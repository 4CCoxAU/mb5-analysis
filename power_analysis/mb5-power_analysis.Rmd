---
title: "Appendix: Power Analysis"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
library(lmerTest)
```

## Design


For this power analysis we will simulate 20 labs contributing 32 infants (640 participants) from 3 to 15 months of age.

Notes: MB1 overall effect size was 0.29 for the single-screen central fixation (CF) method, with additional effect of 0.21 for HPP, and eye-tracking (ET) yielding a slight (non-significant) decrease in effect of -0.06.
We expect to have X labs running infant-controlled familiarization duration, and the other 20-X labs running a fixed familiarization procedure.

Factors:

* *stimulus_type*: indicates the type (complexity/difficulty) of the stimuli that infants are familiarized with during training (high/low stimulus type; within-infant, 12 per type)

* *familiarization_time*: indicates how long each stimulus is exposed during familiarization (5, 10, or 20 seconds; within-infant) GK: the RR says 5, 10, or 15 s -- longest is 15, or 20?

* *trial_num*: indicates the sequential order in which test trials were presented. Trial number thus ranges from 1 to 24. 

* *age_mos*: the infants' age in months (3.0-15.0), centered in *age* column.

* *procedure*: indicates the experimental method that was used to record infantsâ€™ looking to the stimuli: infant-controlled exposure (IC; total familiarization time is achieved over uncontrolled period of time) vs. fixed-duration exposure (FD; controlled period of exposure, unknown period of infant fixation)

* *test_order*: indicates which of the four pseudorandom test orders (from our provided scripts) were used to present test trials to the infant. true for MB5??? 

To do our power analysis, we will generate 1,000 datasets of this structure with a given effect size (e.g., .3), run the mixed-effects regression for each simulated dataset, and count the number of times that the effect is significant. 
Note that we generate normally-distributed looking times, assuming that they have already been log-transformed.

## Simulate Datasets

```{r simulate-data}
set.seed(123) # reproducible sampling

generate_dataset <- function(n_labs=20, n_per_lab=32, 
                             effect_sizes=list(type = .3, familiarization = .1, age = 0, "age*type"=0,
                                               "age*familiarization"=0, "type*familiarization"=0)) {
  # rewrite to use expand.grid ?
  labID = rep(LETTERS[1:n_labs], each=n_per_lab)
  subjID = 1:(n_labs*n_per_lab)

  familiarization_times = c(5,10,20)
  stimulus_types = c(rep("high",4), rep("low",4))
  # trials each subject gets (but randomly ordered)
  fam_by_stim = expand.grid(fam_time = familiarization_times, stimulus_type = stimulus_types)
  
  # assume each lab uses one procedure
  lab_procedure = sample(c("IC","FD"), n_labs, replace=T, prob=c(.5,.5)) # 50/50 IC / FD procedures?
  procedure = rep(lab_procedure, each=n_per_lab)

  test_order = rep(1:4, 5*n_labs) 

  # per-subject data
  simd <- tibble(subjID, labID, procedure, test_order)

  # uniform random vars
  simd$age_mos = runif(nrow(simd), min=3.0, max=15.0)
  simd$age = scale(simd$age_mos, center=T, scale=T)[,1]
  
  # generate per-subject data, put in long (row per-trial) df
  siml <- tibble()
  for(i in 1:nrow(simd)) {
    # randomized trial order (but maybe should be done according to preset pseudorandom orders?)
    tmp_sdat <- fam_by_stim[sample(1:nrow(fam_by_stim), size=nrow(fam_by_stim), replace=F),]
    # need novel and familiar looking times, to calculate prop_novel -- 
    # UNLESS prop_novel turns out to be normally-distributed 
    # (which would be easier to generate, requiring fewer assumptions) - check empirical trial-level data
    siml <- siml %>% 
      bind_rows(tmp_sdat %>%
        mutate(novel_looking_time = rnorm(n = nrow(tmp_sdat), mean=0, sd=1), # = .05
               familiar_looking_time = rnorm(n = nrow(tmp_sdat), mean=0, sd=1), # = .05
               prop_novel = novel_looking_time / (novel_looking_time + familiar_looking_time),
               trial_num = 1:nrow(tmp_sdat)))
  }
  
  siml$trial_num_sc = scale(siml$trial_num, center=T, scale=T) 

  # 12 high / 12 low per child; should be according to 1 of 4 pseudorandom orders, but we're not  actually modeling order effects here so just make blocks:
  siml$trial_type = rep_len(c(rep("high", 12), rep("low", 12)), nrow(siml)) # each

  per_subj_trial_type = c(rep("high", 12), rep("low", 12))
  # add subject random intercept
  siml$subjInt = 0.0
  for(s in 1:length(unique(siml$subjID))) {
    subjInd = which(siml$subjID==s)
    siml[subjInd,]$trial_type = sample(per_subj_trial_type, 24, replace = F)
    siml[subjInd,]$subjInt = rnorm(1, mean=0, sd=1)
  }
  
  # add lab random intercept
  siml$labInt = 0.0
  for(lab in labID) {
    labInd = which(siml$labID==lab)
    siml[labInd,]$labInt = rnorm(1, mean=0, sd=1) # could increase per-lab variability ..
  }
  
  trial_type = with(siml, ifelse(trial_type=="same", 0, 1)) 
  error_term = rnorm(nrow(siml), 0, sd=1) + siml$labInt + siml$subjInt 
  siml$looking_time = trial_type * effect_sizes$type + siml$age * effect_sizes$age + trial_type * siml$age * effect_sizes$`age*type` + error_term
  
  siml$subjID = as.factor(siml$subjID)
  # switch from dummy-code to effects code 
  siml$stimulus_type = as.factor(siml$stimulus_type)
  siml$trial_type = as.factor(siml$trial_type)
  contrasts(siml$stimulus_type) = contr.sum(2)
  contrasts(siml$trial_type) = contr.sum(2)
  return(siml)
}

```

## Plot Example Dataset

We generate and plot an example dataset with trial_type main effect size of .3, age main effect size of -.2, and an age*trial_type interaction effect size of .3.

```{r, fig.width=6, fig.height=4.5, caption="Log(looking time) by trial type and trial number with a simulated main effect (Cohen's d=.3) of trial type, age effect (d=-.2), and trial type * age interaction (d=.3). Shaded regions denote bootstrapped 95% confidence intervals."}
#siml = generate_dataset(effect_sizes=list(type = .3, age = 0, "age*type"=0))
siml = generate_dataset(effect_sizes=list(type = .3, age = -.2, "age*type"=.3))

dag <- siml %>% group_by(subjID, trial_type, age_mos) %>%
  summarise(looking_time = mean(looking_time)) %>% 
  group_by(trial_type, age_mos) %>%
  tidyboot::tidyboot_mean(looking_time) # quite slow..

pos = position_dodge(width=.2)
ggplot(dag, aes(x=age_mos, y=mean, group=trial_type, color=trial_type)) + 
  geom_point(aes(y=mean, x=age_mos), pos=pos) + 
  ylab("Standardized log(looking time)") + xlab("Age (months)") + 
  geom_linerange(aes(ymin=ci_lower, ymax=ci_upper), pos=pos) + 
  theme_bw() + geom_smooth(method="lm")
```


## Model Structure

Infants' log(looking time) (DV) ~ 1 + familiarization order (ABB vs ABA) * trial_type + age * trial_type (same rule vs different rule at test) + experimental_method (HPP vs central fixation vs eye-tracking) * trial_type + multilingual_exposure * trial_type + trial_num * trial_type + (trial_num*trial_type | subject) + (test_order | lab)

```{r model}
# m1 <- lmer(looking_time ~ 1 + trial_type * 
#              (stimulus_type + age + procedure + multilingual_exposure + trial_num) +
#              (trial_num * trial_type | subjID) + (test_order | labID), data=siml)

# model without age
fit_simple_model <- function(siml) {
  m1 <- lmer(looking_time ~ 1 + trial_type * trial_num_sc + (1 | subjID), data=siml)
  return(summary(m1)$coefficients["trial_type1","Pr(>|t|)"]) # "Estimate","t value",
} # trial_type1 = different

# check both
fit_model <- function(siml) {
  m1 <- lmer(looking_time ~ 1 + trial_type * trial_num_sc + trial_type * age + (1 | subjID) + (1 | labID), data=siml)
  sig =c(summary(m1)$coefficients["trial_type1","Pr(>|t|)"],
       summary(m1)$coefficients["trial_type1:age","Pr(>|t|)"])
  return(sig) # "Estimate","t value",
}

# need to update fit_model to return significance of all desired effects (e.g., if effect_size$age!=0)
```

## Power Analysis

We use this simplified model for the power analysis:
y ~ 1 + trial_type * trial_num + trial_type * age + (1 | subjID) + (1 | labID)

To do the power analysis, we simply generate 1000 datasets with main effect sizes of 0.1, 0.2, and 0.3 for trial type, age, and their interaction, run the above linear mixed-effects model, and report how many times 1) the trial type main effect and 2) the trial type * age interaction is significant.

```{r, power-analysis, message=F, warning=F}
# repeatedly generate data and  significance of trial_typesame
get_power <- function(effect_sizes, N=100, alpha=.05, verbose=F) {
  p = data.frame(type=numeric(), "age*type"=numeric())
  colnames(p) = c("type","age*type")
  for(i in 1:N) {
    p[i,] = fit_model(generate_dataset(effect_sizes=effect_sizes))
  }
  if(verbose) {
    print(paste(length(which(p$type<alpha)), "of",N, "simulations had p <",alpha, "for trial type"))
    print(paste(length(which(p[,"age*type"]<alpha)), "of",N, "simulations had p <",alpha, "for age*trial type"))
  }
  return(p)
}

N = 1000
pvalues_pt1 = get_power(effect_sizes=list(type = .1, age = .1, "age*type"=.1), N=N)

pvalues_pt2 = get_power(effect_sizes=list(type = .2, age = .2, "age*type"=.2), N=N)

pvalues_pt3 = get_power(effect_sizes=list(type = .3, age = .3, "age*type"=.3), N=N)
```

### Effect sizes = .1
`r paste(length(which(pvalues_pt1$type<.05)), "of",N, "simulations had p <",.05, "for trial type.")`
`r paste(length(which(pvalues_pt1[,"age*type"]<.05)), "of",N, "simulations had p <",.05, "for age*trial type.")` 
<!--(With 16 infants per lab (total N=320): 316 of 1000 simulations had p < 0.05 for trial type. 1000 of 1000 simulations had p < 0.05 for age*trial type.)-->

### Effect sizes = .2
`r paste(length(which(pvalues_pt2$type<.05)), "of",N, "simulations had p <",.05, "for trial type.")`
`r paste(length(which(pvalues_pt2[,"age*type"]<.05)), "of",N, "simulations had p <",.05, "for age*trial type.")` 
<!--(With 16 infants per lab (total N=320): 777 of 1000 simulations had p < 0.05 for trial type. 1000 of 1000 simulations had p < 0.05 for age*trial type.)-->

### Effect sizes = .3
`r paste(length(which(pvalues_pt3$type<.05)), "of",N, "simulations had p <",.05, "for trial type.")`
`r paste(length(which(pvalues_pt3[,"age*type"]<.05)), "of",N, "simulations had p <",.05, "for age*trial type.")` 
<!--(With 16 infants per lab (total N=320): 985 of 1000 simulations had p < 0.05 for trial type. 1000 of 1000 simulations had p < 0.05 for age*trial type.)-->

For context, .25 is the average effect size from the meta-analysis of rule learning, and .3 is the average effect size across all published developmental experiments.
Thus, the latter two power simulations probably pertain in our case.